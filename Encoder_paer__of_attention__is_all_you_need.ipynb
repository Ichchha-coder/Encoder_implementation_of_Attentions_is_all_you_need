{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_cu88cJjCwI"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n"
      ],
      "metadata": {
        "id": "ZYrwJcQxjVOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
        "\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = torch.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, v)\n",
        "\n",
        "        return output, attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
        "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
        "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
        "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
        "\n",
        "        residual = q\n",
        "\n",
        "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
        "\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "\n",
        "        output, attn = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
        "        output = self.dropout(self.fc(output))\n",
        "        output = self.layer_norm(output + residual)\n",
        "\n",
        "        return output, attn\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Linear(d_in, d_hid)\n",
        "        self.w_2 = nn.Linear(d_hid, d_in)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_in)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = self.w_2(torch.relu(self.w_1(x)))\n",
        "        output = self.dropout(output)\n",
        "        output = self.layer_norm(output + residual)\n",
        "        return output\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
        "\n",
        "    def forward(self, enc_input, slf_attn_mask=None):\n",
        "        enc_output, enc_slf_attn = self.slf_attn(\n",
        "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
        "        enc_output = self.pos_ffn(enc_output)\n",
        "        return enc_output, enc_slf_attn\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, n_src_vocab, d_model, n_layers, n_head, d_k, d_v, d_inner, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_word_emb = nn.Embedding(n_src_vocab, d_model)\n",
        "        self.position_enc = nn.Parameter(torch.zeros(1, 512, d_model), requires_grad=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer_stack = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src_seq, src_mask):\n",
        "        enc_output = self.src_word_emb(src_seq) + self.position_enc[:, :src_seq.size(1), :]\n",
        "        enc_output = self.dropout(enc_output)\n",
        "\n",
        "        enc_slf_attn_list = []\n",
        "\n",
        "        for enc_layer in self.layer_stack:\n",
        "            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n",
        "            enc_slf_attn_list += [enc_slf_attn]\n",
        "\n",
        "        return enc_output, enc_slf_attn_list\n"
      ],
      "metadata": {
        "id": "PiZPoru0jYvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"This is a sample sentence\",\n",
        "    \"Transformers are very powerful models\",\n",
        "    \"This is another example\"\n",
        "]\n",
        "\n",
        "# Build a simple vocabulary\n",
        "vocab = set(\" \".join(sentences).split())\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# Convert sentences to token indices\n",
        "def sentences_to_tensor(sentences):\n",
        "    return torch.tensor([[word2idx[word] for word in sentence.split()] for sentence in sentences])\n",
        "\n",
        "# Prepare data\n",
        "data = sentences_to_tensor(sentences)\n"
      ],
      "metadata": {
        "id": "HQ2FVpKEjZm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_src_vocab = len(vocab)\n",
        "d_model = 512\n",
        "n_layers = 6\n",
        "n_head = 8\n",
        "d_k = d_model // n_head\n",
        "d_v = d_model // n_head\n",
        "d_inner = 2048\n",
        "\n",
        "model = TransformerEncoder(n_src_vocab, d_model, n_layers, n_head, d_k, d_v, d_inner)\n"
      ],
      "metadata": {
        "id": "Dwo6j1hpjeia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_mask = None  # In this simple case, we do not use a mask\n",
        "output, attn_list = model(data, src_mask)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Attention shapes:\", [attn.shape for attn in attn_list])\n"
      ],
      "metadata": {
        "id": "ZEYFh-52jhMM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}